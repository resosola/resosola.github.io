# kubelet内存泄漏问题解决
## 背景
在使用kubelet 1.29.1版本时，发现API Server不可达会导致内存泄漏，若持续过长时间的不可达，最终会导致机器挂掉。

## 问题分析
机器内存使用情况：
```bash
free -h
              total        used        free      shared  buff/cache   available
Mem:           62Gi        57Gi       415Mi       111Mi       5.1Gi       4.7Gi
Swap:         8.0Gi       4.0Mi       8.0Gi
```

先通过top命令查看各进程内存占用情况，发现大部分用户态进程占用内存在 0.0 ~ 0.3% 之间，看起来都不是内存占用过大的元凶：
```bash
    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
   1096 mysql     20   0 1295840 174540  13308 S   0.0   0.3  16:21.59 mysqld
    792 root      20   0  134096 120756 115720 S   0.0   0.2   0:10.21 xxxxx2
   3246 root      20   0  464276  92520  17460 S   0.0   0.1   2:51.43 xxx123
   3349 root      20   0  731976  90392  18340 S   0.0   0.1  84:13.32 xxx23
   3284 root      20   0  733528  78160  11724 S   0.0   0.1  75:59.21 xxxxx1
    829 root      20   0  639796  49324  30764 S   0.0   0.1   0:00.60 node
   1220 root      20   0 2244656  47660  29728 S   0.0   0.1  10:38.93 dockerd
    887 root      20   0 3132056  41208  25100 S   0.3   0.1  76:07.29 containerd
   1395 root      20   0  751920  35276  25360 S   0.3   0.1 189:18.14 containerd
    780 root      20   0 3146500  34500  16032 S   0.3   0.1 142:33.21 snapd
    783 root      20   0 2196560  27036  23512 S   0.0   0.0  58:17.81 xxxx1
    497 root      19  -1   86392  23736  22348 S   0.3   0.0  37:16.91 systemd-journal
   1165 root      20   0   66500  20472   6672 S   0.0   0.0   2:06.89 vpnkit
   3384 root      20   0  582424  19484  14504 S   0.0   0.0  51:25.18 xx23
    652 root      rt   0  280132  17944   8204 S   0.0   0.0   1:50.41 multipathd
    763 root      20   0  259256  17360  14812 S   0.0   0.0   0:36.90 NetworkManager
    883 root      20   0  107908  16540   9220 S   0.0   0.0   0:00.08 unattended-upgr
    774 root      20   0   29328  14056   6456 S   0.0   0.0   0:00.09 networkd-dispat
    807 root      20   0  392400  11484   9676 S   0.0   0.0   0:02.50 udisksd
      1 root      20   0  167664  11476   8192 S   0.0   0.0  28:38.31 systemd
 223490 systemd+  20   0   24304  10888   6472 S   0.0   0.0   3:55.08 systemd-resolve
   3244 root      20   0  310484  10336   6988 S   0.0   0.0   5:48.26 xxxxx12
2549707 root      20   0   15960   9688   8060 S   0.0   0.0   0:00.01 sshd 
```
怀疑是内核占用过多内存，查看meminfo文件发现果然如此，且内核的 Slab（特别是 SUnreclaim）占用了巨量内存：
```bash
cat /proc/meminfo
MemTotal:       65833796 kB
MemFree:          425000 kB
MemAvailable:    4926556 kB
Buffers:          659068 kB
Cached:          1306020 kB
SwapCached:          356 kB
Active:          1045124 kB
Inactive:        1472028 kB
Active(anon):     298556 kB
Inactive(anon):   375672 kB
Active(file):     746568 kB
Inactive(file):  1096356 kB
Unevictable:       18468 kB
Mlocked:           18468 kB
SwapTotal:       8388604 kB
SwapFree:        8384508 kB
Dirty:               644 kB
Writeback:             0 kB
AnonPages:        570244 kB
Mapped:           383976 kB
Shmem:            113960 kB
KReclaimable:    3389184 kB
Slab:           61672216 kB
SReclaimable:    3389184 kB
SUnreclaim:     58283032 kB
KernelStack:        9024 kB
PageTables:         6736 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    41303452 kB
Committed_AS:    3563880 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       66328 kB
VmallocChunk:          0 kB
Percpu:           981248 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       2
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:            4096 kB
DirectMap4k:     6639068 kB
DirectMap2M:    60370944 kB
DirectMap1G:           0 kB
```


接着使用slabtop命令，查看占用内存最多的 ​​Slab 对象类型，发现是kmalloc-512：
```bash
Active / Total Objects (% used)    : 47772052 / 65894040 (72.5%)
 Active / Total Slabs (% used)      : 1785617 / 1785617 (100.0%)
 Active / Total Caches (% used)     : 101 / 154 (65.6%)
 Active / Total Size (% used)       : 13652197.28K / 16264972.28K (83.9%)
 Minimum / Average / Maximum Object : 0.01K / 0.25K / 15.14K

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
15526050 15525485  99%    0.13K 517535       30   2070140K kernfs_node_cache
14485664 14485664 100%    0.50K 452677       32   7242832K kmalloc-512
11604608 3222835  27%    0.06K 181322       64    725288K kmalloc-64
5609562 154695   2%    0.19K 133561       42   1068488K cred_jar
4392512 201043   4%    0.25K 137266       32   1098128K filp
3088134 3071654  99%    0.19K  73527       42    588216K dentry
3013712 3012961  99%    0.59K 115912       26   1854592K inode_cache
2418112 2416908  99%    0.06K  37783       64    151132K kmalloc-rcl-64
1546720 1541367  99%    0.57K  55240       28    883840K radix_tree_node
917248 917248 100%    0.03K   7166      128     28664K kmalloc-32
```
使用命令监控kmalloc-512，发现确实在不断快速增长：
```bash
watch -n 1 'cat /proc/slabinfo | grep kmalloc-512'
```

某些用户态进程的行为（比如频繁打开文件、创建网络连接、使用容器、调用系统 API）会触发内核分配大量 slab 对象，如果这些对象没有被及时释放，就会造成 Slab 堆积！​接下来则打开一个窗口进行监控，打开另一个窗口排查导致内存泄漏的进程，具体方法是不断试着关闭可疑进程，看kmalloc-512是否还在快速增长，如果没快速增长则进程应该就是元凶了。

经过排查，发现是kubelet引起的，停止进程后kmalloc-512增长速度显著降低。

查看kublet日志，发现是其无法连接到 Kubernetes API Server引起的，并且会不断重试。

​​kubelet每隔几秒就会尝试访问 API Server 获取或上报关键信息（比如节点状态、租约、服务列表、存储驱动、运行时类等）。由于 API Server当前不可达，请求会​​不断发起​、​​不断超时​，每次超时都会触发一系列内核对象分配（如网络连接、请求结构体、响应缓存、元数据等）​，这些对象很多是 512 字节左右，走 kmalloc-512分配路径​。​​由于请求持续失败，对象不断分配但从未释放 → 导致 kmalloc-512持续增长。 后续保证API Server可达后该内存问题即解决了。